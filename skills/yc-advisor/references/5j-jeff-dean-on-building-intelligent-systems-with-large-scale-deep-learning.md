# Jeff Dean on building intelligent systems with large scale deep learning

**Author:** Jeff Dean
**Type:** Video
**URL:** https://www.ycombinator.com/library/5j-jeff-dean-on-building-intelligent-systems-with-large-scale-deep-learning
**YouTube ID:** HcStlHGpjN8

---

Jeff Dean is a Google Senior Fellow in the Research Group, where he leads the Google Brain project.

He spoke to the [YC AI](https://blog.ycombinator.com/yc-ai/) group this summer. Here are his slides:

{{<https://www.scribd.com/embeds/355752799/content?start_page=1&view_mode=scroll&access_key=key-KSLxDZmNLXd1onGghUsp&show_recommendations=true}}>

## Transcript

so I'm going to tell you a very not super deep into any one topic but very broad brush sense of the kinds of things we've been using deep learning for the kinds of systems we've built around making deep learning faster and this is joint work with many many many people at Google so this is not purely my work but most of it is from the Google brain team which I lead and so the brain teams mission is basically makes machines

intelligent and then use that new capability to improve people's lives in a number of different ways and the way we do this is we conduct a long-term research kind of independent of any particular application in of and probably supposed to stand in one place independent of any particular application we build an open-source systems that help us with our research and deploying of machine learning models like tensorflow

we collaborate across Google and all of alphabet in getting machine learning systems and research that we've done into real Google products so we've done a lot of work in like Google search Gmail photos speech recognition translate and you know many other places we also bring in a lot of people into our group through internships and a new residency program that we started last year for people who want to learn how to do deep learning research and that's going to pretty successful program as

well so the main research area is that our group is working in are these I'm going to focus mostly on these today actually a little bit of perception too but so in January I put out a blog post that kind of just highlighted some of the work our group has done over the last over 2016 and in putting that together I kind of realized we were doing a lot of different things so the nice thing about this is each one of these blue links is a link to something

kind of interesting and substantial like a research paper or a product launch using learning or some new tensorflow features we've we've added so I won't go through that all now but you can go find that blog post and learn more about some of the stuff we've done up to okay so why are we here you probably already all know this given that you're working on AI related companies as I understand it but the field of deep learning and

neural networks in particular are really causing a shift in how we think about approaching a lot of problems and I think it's really changing the kinds of machine learning approaches that we use in the 80s and 90s it was the case that neural Nets seemed interesting and appealing but they weren't the best solution at the time for a lot of problems we cared about because they just didn't quite have enough training data enough computational capabilities and so people used other methods or

developed kind of shallower machine learning methods with much more hand engineering your futures and if you fast forward to now what's happened is we've got much much more compute I actually did an undergrad thesis in 1990 on parallel training of neural labs because I liked appealing attraction of the neural net model and I thought if we could just get like you know a bunch more compute by paralyzing over a you know a 64 processor hypercube machine that it would all be even better it turned out what we needed was like a

hundred thousand times as much compute not sixty times but if you fast forward to today we actually have that and so what's happened is we now actually have you know the case where neural apps are the best solution for an awful lot of problems and a growing set of problems where we either previously didn't really know how to solve the problem or where we could solve it but now we can solve it better with known apps so the talk is really meant to orient you across a whole bunch of different problems where

this is the case so growing use of deep learning so really our group started in order to investigate the hypothesis that large amounts of compute could actually solve interesting problems using your labs and so when we first started you know they weren't we were sort of the vanguard of people using no labs at Google we did a bunch of work on unsupervised learning at a really large scale using at that time we didn't even have GPUs in our data centers so we just

used 16,000 GPU cores and we did kind of interesting things with unsupervised learning there but gradually we kind of built tools that enable people to use fly machine learning and deep learning in particular to a lot of problems and you can see the growth rate of you know this is directories containing model description files either from our first generation system and starting in about 2015 our second generation system tensorflow and we've deployed machine learning and in collaboration with lots

of teams other teams have also been independently just picking up this idea of deep learning and using in lots and lots of places in Google products and that's why you see that growth rate and it's continuing to go out one of the things we focus on a lot is how can we reduce experimental turnaround time for our machine learning experiment and because there's a very different qualitative feel to doing science and research in a domain where an experiment takes a month versus doing it in a

domain where you know minutes or hours you get you know an answer and then you can figure out what the next set of experiments are that you want to run so a lot of our focus is on scaling machine learning models and scaling the underlying infrastructure and systems so that we can actually for some problems approach minutes or hours rather than weeks or months so part of that has been building the right tools so tensorflow is kind of our second generation system that we built for tackling deep learning problems and machine learning problems

the first one we did an open-source the second one we said we should really fix some of the design problems we saw on our first system keep the good features about it and then design this from the start to be an open source platform so that people all over the world not just at Google can benefit from from this and can help build a community that can all contribute to and improve the system Zac stoned here is our tensorflow product manager

extraordinaire and is doing a great job of building the community building inside Google and outside so the goals of tensorflow that we want to establish this common platform for expressing all kinds of machine learning ideas so something that can be used for deep learning can be used for other kinds of machine learning that can be used for tackling perception problems and you know language understanding problems and what if you have a crazy new machine learning research idea that doesn't really fit

into what people have done before we want it to be at least expressible relatively easily in tensorflow and then we want to make that platform really great for research but we also want to be able to take that something you've developed in tensorflow maybe experimentally and then if you now want to deploy that in a production setting run it at the data center run it at scale running on a phone all these kinds of things we want that to be another something you can do in the sense of a

framework and by open sourcing it we make it available to everyone so how has this been going well so this is a comparison of github stars of which is one metric of popularity or interest in different source code repositories on github and I show you a comparison of tensorflow with a bunch of other open-source machine learning packages many of which have been around for for you know many more years of intensive

attention flows this brown line up going up fairly steeply so this has been pretty good I think the the reception for what tensorflow does which is enable flexible research but also this kind of production readiness and being able to run in lots of places is pretty appealing and if you look at the other open source packages which we did when we were starting to work on tensorflow you know many of them have two of the three attributes that we care about being able to be really flexible scalable and

sort of run on any platform and they all have different emphases but we wanted something that satisfied all three of those so that's kind of cool we've been focusing a fair amount on speed I think when we first released tensorflow we released a bunch of really nice tutorials that showed how to do different things with tensorflow but one of the mistakes we made was we released code that was meant to be exposed Tory and clear and not necessarily the highest performance way

you would write that but often then people would take that as the way you should write a high performance tensorflow model and that wasn't necessarily a case so we're now adapting and trying to put out things that are both the best way from a clarity standpoint but also our high performance that's been a bit of a tentacle guide I think a bit of a bad rap but actually our performance is quite good so we've been doing a bunch of benchmarking and producing reproducible benchmark results

that shows that our scaling is quite good so this is single machine scaling nearly linear speed-up for a bunch of different image models on up to 8 GPU cards pretty close to linear speed-up for 64 GPU cards for a bunch of different kinds of problems so don't don't if you hear tensorflow slow don't don't believe it we also support lots of different platforms and I think this is important because often you want to train a model

on a large data set in a data center but then deploy that on a phone and so we run on you know iOS and Android and raspberry PI's but also CPUs and if you have a GPU card or a TV you cards we're happy to use that we also run on our custom machine learning accelerators that I'll talk about in a minute but really we want to run on everything so there's a bunch of other device manufacturers that are developing kind of funky mobile ml accelerators or Qualcomm has a DSP and they're all working to make sure the tempter flow

runs well on those devices we also kind of want to be agnostic of language that people want because you want to be able to run machine learning where it makes sense and different people have different sort of language environments most the most fully developed system is obviously Python but the C++ friend works pretty well for production use and then a bunch of other people of external community members have added a variety of other kind of not fully fleshed out but reasonable

support for some of these other languages we have a pretty broad usage base so like a year ago we had a almost a year ago we had a meeting at Google of people using tensorflow and it was pretty impressive we had people from most of these companies in the room which I think normally they don't all get in a room together places like Apple is actually there as well and Vidya Qualcomm uber Google snapchat you know many men until many many other places so

terms of stars you know I showed you the graphs related to machine learning platforms this is the top repositories on github overall and we're up to number six which is pretty good and all the other ones are either Java Script or a list of programming books this is a visualization of where people are interested in different github repositories which is kind of cool machine learning is done all over the world so one of the things that's

happened as that growth in interest has happened is there's been a pretty broad set of external control contributors and so there's really you know I think we're up to almost a thousand non-google contributors across the world doing all kinds of different things for adding features or fixing bugs or improving the system in various ways which is been really nice oh and I think it's kind of nice that there's growing use in machine learning classes of tensorflow as a way of illustrating machine learning concepts so at really good machine learning

University like Toronto Berkeley Stanford in other places they're starting to use that as the core of their curriculum okay so now I'm going to switch gears a bit and talk about some sort of more product oriented applications of deep learning at Google Google photos is a good example obviously computer vision that works and one thing you could do is you know make a photos product around the idea that you can actually understand what's in people's photos and that's been going really well as a lesson for for you who

are starting companies often applied domains I think it's really important to be able to look at the machine learning work that is happening in the world and realize that Austin you can reuse many of the same ideas from one domain and just by pointing at it kind of different data sets get completely different interesting product features so if you for example use the same basic model structure training on different data and you get something different one general model trend is given an

image predict interesting pixels so there's a bunch of you know ways you could do that but if you have a model structure that does that my summer intern from a few years ago Matt velar who actually went off to found clarify which is a computer vision company we were working in collaboration with the street view team on identifying text in Street View images and so to do that you can have training data where people have circled or drawn boxes around text and

you just try to predict the heatmap of which pixels contain text in a Street View image and so this works reasonably well and then you can run an OCR model on those pixels and actually read the text and it works you know across lots of different font sizes and colors and whether it's close too far from the from the camera and so then some people in the Maps team decided they would build this thing that would help you identify whether your rooftop has solar energy

potential and how much energy you could generate by installing solar panels and so obviously one of the first things you have to do is find rooftops and that's exactly the same model but just with different training data where you now have circles around rooftops and then there's a bunch of other work to estimate the angle of the rooftop from the imagery or multiple views of the same same house and then some stuff to predict you know what is the solar energy potential for that in another area where we've applied this is in the

medical domain so the same basic model we want to be able to say take a medical imaging problem one of the first ones we've been tackling is ophthalmology problems and in particular taking a retinal image like this and tackling deciding whether or not this has symptoms of a degenerative disease called diabetic retinopathy and so this is again the same kind of problem you want identify parts of the eye that are related to you know that seem to be diseased in some way and then you also

have a whole image classification problem of does this eye show symptoms at the level of one two three four or five and turns out you can do this so some people in our group have done a really nice sort of medical study showing that if you collect 150,000 ophthalmology images and then you get each one labeled by seven ophthalmologists because if you ask two ophthalmologists degrade the same image one two three four or five they agree 60

percent of the time - slightly terrifying if you ask the same ophthalmologist to grade the same image a few hours later they agree with themselves sixty five percent of the time and and that's mildly terrifying so we had to get every image labeled by seven ophthalmologists to reduce the variance in the score and say oh it's five people think it's a two so it's probably more like a tooth and a three but in any case the the punchline of this paper is we now have a model that performs on par slightly better than the median of eight US board-certified

optimal which is cool because there's a bunch of places in the world especially in India and other countries where there are you know many people at risk and there just aren't enough ophthalmologists so actually doing clinical trials in India we've licensed this our verily subsidiary whose license it to a camera an ophthalmology camera manufacturer who's going to be integrating this into the actual ophthalmology camera another area where being able to see is pretty useful is robotics if you're trying to build

robots just being able to perceive the world around you clearly makes things a lot better so we've been doing a bunch of experiments both with real robots and also with simulated robotic environments and also with trying to do imitation learning from people performing actions and then trying to get robots to do so we set up what we call an arm farm oops I'm not playing oh maybe I'm not on the internet well anyway it's not that

exciting except we have a bunch of robots trying to grasp things and they can essentially learn to grat learn on their own whether they're grasping something successfully by just having a bin of things in front of them and they just try to pick something up and if they fail their gripper closes all the way if they succeed then they don't close the gripper all the way and they can actually see from the camera that they've managed to pick something up and so they can practice picking things up and we can pool all the sensor data from

all the robots that are doing this to retrain a model every night for grasping so that the next day's grasping attempts are better and better and by having lots of robots do this you actually get a lot of parallel experience much more than you can get on a single robotic arm and so we have a data set of that we've actually released publicly of about 800,000 grasp attempts versus about 30,000 grasp attempts which is kind of a big public data set in the past and surprisingly has grasped attempts gives

you a much better grasping mechanism and model than 30 of them not surprising we've also been trying to this is me awkwardly looking at a robot on a screen that you can't see doing some actions I'm going to like mimic the robotic nature of it and then we have a video of me doing that and then we're just trying to learn from the videos to transfer that action to the real robots and that's working reasonably well

here's another example and we're doing that first lluvia simulator and then we're taking that simulator and then trying to transfer those activities to a rebel real robot and that works reasonably well as well another place that I'm pretty excited about deep learning is in lots and lots of scientific domains you often have the case where you have a simulator of some really complex

phenomenon and that's often a sort of HPC style application and very computationally expensive but it kind of gives you insight into whatever scientific processes are going on and that allows you to kind of iterate in a computational science methodology but often those computations are pretty expensive and so one of the things we've been working on and this is just one example we have a lot of different fields of science where we've seen this to be true is you can use those simulators as training data for an OLAP

so quantum chemists have a similar they have a problem where they take in a configuration of molecules and they run a bunch of time steps and then at the end they get some information about how the ultimate configuration of those molecules turns out and from that they get a few properties about those molecules like is it toxic did it bind or something else you know a handful of these things so it turns out that's the data that if you use that as input you run this really expensive simulator for

an hour then you get these thirty numbers out that turns out to be great training data for an ER lab and so you can train a neural net to do exactly that same tasks or to approximately that task approximate the entire simulator and you can essentially the punch line is the bottom there you essentially get indistinguishable accuracy from using the real simulator but it's three hundred thousand times faster and that has a lot of implications for how you might do quantum chemistry if you suddenly have something 300,000 times faster you might

like run 100 million things through your through your simulated neural lipase emulator and figure out what's going on look and you know to identify a bunch of candidates that you might want to look into in more detail so that's exciting another place where these kinds of pixel to pixel models come in is some people in Google have done a model that tries to predict depth from an input image and we have some training data where we have

the true depths given a the camera viewpoint and where things are in the room or in the world and then we try to train a model to do predicted depths from just the raw pixels of that image so that's a pixel to pixel learning problem and you can imagine a lot of pixel pixel learning problems and indeed you know one one application and in cameras is you want to predict depth and a portrait and then you can do kind of funky cool effects like identify the person in the foreground and turn the

background black and white or like make it all fuzzy and artsy in the background which is kind of cool but it turns out you can also take microscope microscope images as the raw microscope images input and then the chemically stained microscope image as the target for your model and so for example that's often how people see you know cell bodies and cell boundaries is you apply different kinds of stains to the cells and then

you can make them show up on a microscope better and you can see what's going on well it turns out so this animation that's the input that's the ground truth and that's the predicted output of a neural net that's trained to sort of virtually stain something without actually staining it and this is important because it turns out when you actually stain something that kills the cells so you don't get any temporal information about what's going on in the

cells essentially die when you apply the stain but here you can virtually stain something but then follow them longitudinally in time and see how cell processes kind of continue to happen without actually staining them you can also stain for things that you can't actually really necessarily develop a true chemical stain so if you have some one label which things are axons in which things are dendrites and neural tissue you can have a microscope viewer that highlights axons and dendrites and

different colors and cell bodies even if that's kind of not something that you can chemically do with a real stained one of the areas we've been doing a lot of work is in language understanding models and so this started out as research in our group to do essentially sequence the sequence learning so you have some input sequence and conditioned on that input sequence you want to predict an output sequence so this turns

out to be useful for actually a whole bunch of different problems but one of them is translation so if you have a bunch of sentence pairs one in french and the corresponding meaning sentence in english then you can use a sequence the sequence model to take the input sentence one word at a time or even like one character at a time and then when you hit a special end of french token then you essentially start spitting out the corresponding english meaning in english translation of that scent of

that french sentence and so that worked like this and you have training data that is like that and use try to predict the next word from that train engine using recurrent neural net and that turns out to work reasonably well and then you're actually trying to find the most probable sequence not the sequence with the most probable individual individual terms and so you do a little beam search where you kind of keep a window of candidates and you

sort of search over possible vocabulary items until you are happy and found a likely output sequence and that's how you do translation so one application of this is in Gmail we had we added a feature called Smart reply where essentially we get an incoming email so this is one sent to my colleague Greg Corrado from his brother hi we want to invite you to join us for Thanksgiving dinner please bring your favorite dish RSVP go next week so to reduce the

computational cost we have a small feed-forward neural net that says is this the kind of thing where a small short reply would make sense and if yes then we going to activate a sequence the sequence model and we're going to do a much more computationally expensive thing with that in messages input and then we're going to try to predict plausible replies and so this system produces three it says count us in will be there or sorry we won't be able to make it and so this is a nice application of sequence of sequence

models and if you squint in the world you'll find lots of applications of these and so turns out to my reply in April 2009 there was an April Fool's joke that Google put out saying haha we're going to reply to your email automatically huh but then in November 2 2015 we launched this as a real product and in just three months 10 percent of mobile inbox replies are generated by the smart replies so that's kind of cool but

obviously one of the real potential applications of this was Translate which is what we were doing demonstrating that this research was effective on on a large by academic standards but smallish public data set of translation data called WMT so when we look to work on applying this to the real Google Translate product we actually had a hundred X to a thousand X as much training data and so scaling this up was actually pretty challenging and we

wanted to make the model a lot higher quality but we did a nice fairly detailed write-up of the engineering behind that in this many many out there paper and so this is kind of the structure of the model that we came up with it has a very deep lsdm stack each of which runs on different GPU there's an attention module so that rather than just having a single state that's updated by the recurrent model you keep track of all the states and then you learn to pay attention to different

parts of the input data when you're generating different parts of the output sequence so you're about to generate you know the next word and you look back at the word hello and the input sentence and so on and so this thing runs on one replica of this model runs on a machine with a GPU cards with different pieces of it in different places and then we run a lot of copies of this model to do data parallelism across the large training data and we share the

parameters so this is a technique we've been using for quite a while that we originally published in 2012 about what we call at that time a parameter server and then using many parallel data data parallel copies to process different input data all trying to update those shared parameters by applying gradients to those parameters and this allows you to scale training quite quickly so you can have you know 50 replicas of this kind of setup for 20 I think in this

case we were using about 16 so we're using 100 GPU cards to train a model and the really good news is the blue line here is the old phrase based machine translation system they didn't really have much machine learning it any machine learning in it had large statistical models for lots of different sub pieces of problem so it had a a target language model that told you how often every five word sequence in English occurred it had an alignment model that says how words in English and

French sentences align had a phrase table and a dictionary of plausible english and french phrases and sentences and it was like five hundred thousand lines of code to glue this whole thing together and that's the blue line and what we're showing is the quality of translations generated by that system as judged by humans and the green line has a substantial jump in quality for basically nearly every language pair

jumps up very substantially it doesn't look like much but those are really big jumps and quality and the other nice thing is that system is 500 lines of tensorflow code instead of 500 thousand lines of GUI code with like lots of handwritten logic and the greet the yellow line on top is human bilingual human not professional translator but some of the speaks both those languages translations as judged by other humans and so you can see that for some

language Spurs were actually getting quite close to that human level quality for translation which is pretty exciting and when we we were trying to kind of roll this out slowly across lots of different language pairs and so we launched it in the dead of night in Japan and all of a sudden all of Japan kind of met many people in Japan noticed that suddenly English to Japanese translation was actually usable in quality as opposed to before but when it was kind of supported but not usable as

one of the people on our translate team referred to it and so this professor at a Japanese university decided we did this experiment translating the first paragraph of Hemingway's the snows of Kilimanjaro to Japanese in the back and see what the quality looked like and so if we focus on the last sentence the old phrase day system says whether the leopard had what the demand that that altitude there is no that nobody explained so I think there's a leopard involved other than that I really can't understand that and neural

machine translation just generates much more natural sounding translations so no one can explain what leopard was seeking at that altitude and the only mistake it made was it left out the words up so you can see how this transforms it from like really not usable to like actually pretty good another area we're doing a lot of research in is this notion of automating solution of machine learning problems what we call learned to learn and the idea here is that the current way you

solve a machine learning problem probably many of you your companies are solving machine learning problems you have data you have some way of doing lots of compute a bunch of GPU guard do something and then you have a human machine learning expert saying okay I'm going to try this kind of model use this learning rate and I'm going to do transfer learning from this data set and and then you hopefully get a solution what we'd like to turn that into is you have data and maybe use a hundred times as much compute but you don't need a human machine learning expert and if we

could do that that would be really really good because if you think about what's happening in the world you know there's probably 10 million organizations in the world that should be using machine learning and actually have probably data in electronic form that it would be suitable for machine learning but there's you know order a thousand organizations that have really hired machine learning experts in the world actually tackle some of these problems so we're trying lots of different efforts in this area and I'll talk about two of them one is a way of

designing neural architectures automatically and the other is a way of learning optimizers automatically so architecture search the idea is we want to have a model generating model so the same way a human machine learning expert says I'm going to try this kind of model we're going to have a model generating model that's going to spit out models for this problem to solve to tackle a particular problem and so the way this will work is we're going to generate ten model architecture's we're going to train each of them for a few hours and then we're

going to use the loss of the generative models as a reinforcement learning signal for the model generating model and this is sort of just on the realm of feasible for small problems today but it actually works for small problems so here is an example of a model architecture came up with and you'll see it looks sort of not like something a human would have designed the wiring is kind of crazy and this is see farc n which is a very small color image

problem with ten different classes got pictures of horses and planes and cars not that many classes but it's been pretty well studied in the machine learning literature and the error rate like all machine learning image problems is been dropping over the years but everything above these last four lines is a human generated machine learning expert model that someone came up with a new thing and published and beat the previous state of the art and so this is the current state of the art and this neural architecture search basically

with that architecture got very very close to that state of the art without any human sort of knowledge of the underlying architecture we also tried it on a language modeling task and the traditional way you do this for recurrent models is using lsdm cell whose structure is shown there that's kind of the default thing you're going to do if you're going to use any sequence data and we just gave the architecture search the sort of

underlying primitives of an LCM cell and said go to it find us some way of dealing with sequential data and that's the salic came up with it looks somewhat different but in this case it actually beat the state-of-the-art by a pretty substantial margin for this language modeling task and the other interesting things we then took that cell and used it on a completely different sequential task in medical records future prediction tasks and it performed better

than the metal stem cell in that domain as well so learning the optimizer rule is similar we're going to have symbolic expressions with and give it the model the the optimizer of expression learning model access to the raw primitives that you might consider using in a neural optimizer update rule things like here's the gradient here's the running average of the recent gradients here's the momentum term and so the top four lines

here are human designed update rules that people traditionally use and then they've been designed over the last decade or few decades in case of SGD and are generally what people use Adam as a pretty good choice these days and often SGD with momentum which is the second line is the best choice and what you see is that this thing came up with 15 or something completely different

expressions than what we've explored and they're almost all better than all of the human design ones and so that's kind of encouraging that that's going to appear in ICML and bumpy goop and we also took one of the most promising ones of those and we then transferred it to a different problem where we hadn't the problem we didn't design the optimizer on and we use this other optimizer and found that it gave you no better

training for flex it ease lower is better for publicity and better loose core which higher is better for that metric than Adam which was the best optimizer we'd found before so I think this whole motion of learning to learn is going to be pretty powerful because a lot of what machine learning experts do when they sit down to solve a problem is actually they run lots of experiments and right now a human can't run that many experiments right it's just a lot of cognitive load to run 50 experiments

or 100 experiments and this thing can run you know twelve thousand experiments in a weekend and many of them suck but many of them don't so the other thing is interesting is that a lot of what's happened is we've been able to solve lots of problems because we have a lot of data and because we've been able to scale the amount of compute we throw at different problems and so really one of the nice properties of deep learning has is really transforming how we think about

designing computers these days so deep learning has two really nice properties so one is that it's perfectly tolerant of very reduced precision arithmetic so you know one significant digit kind of thing you don't need double precision you certainly you don't need single precision floating point and the other property it has is it's generally made up all the algorithms I've showed you are made up of a handful of specific operations kind of cobbled together in different ways and so that really leads

to an opportunity where if you can build custom machine learning Hardware targeted at doing very reduced precision linear algebra then you can all of a sudden unlock huge amounts of compute relative to CPUs or GPUs or you're not really targeted at doing these kinds of things and so this is we've been doing custom machine learning accelerators for a while we've had a first generation one that was targeted at speeding up inference so not training but inference

when you're actually running a trained model in in the context of a product we had our first version deployed in our data center for two and a half years or something and we just revealed this system which is designed for both training and inference at Google i/o and this is a board one of the things we felt was important was to design not just a chip for training but also an entire system so because you're

unlikely to get enough compute for large problems on a single chip so we designed a really high performance chip and we also designed them to be hooked together so this is what we call a pod which is 64 of these Bowl towards each of which has four chips so 256 chips and that's eleven and a half petaflop so compute and we're going to have lots and lots of these in our data centers which is pretty exciting because I think we'll be able to tackle much bigger problems it's going to bring a

lot more compute for someone to learn to learn approaches and normally programming a supercomputer is kind of annoying so we decided we make these programmable via tensorflow so you essentially can express a model with a new interface that we're adding the tensorflow 1.2 called estimators and then the same program will run with minor modifications on CPUs GPUs or on GPUs and that's going to be available through Google Cloud you can get a thing called a cloud TPU later this year which

is going to be a virtual machine we have 180 teraflop TPU version 2 device attached and it will run tensorflow program super fast we hope we're also making a thousand of these devices available for free to researchers around the world who are doing interesting work and want more compute and are committed to actually publishing the results of that work openly and also hopefully giving us feedback about what's working well on these TPU devices and what's not

and ideally open sourcing code associated with those models but not we're not sure that's going to be a hard requirement but as desire on our part to help sort of speak up the whole science and machine learning research ecosystem and so you can sign up there if you're interested in any of these things cloud Google cloud is also producing higher level api's that are more managed services or pre trained models that you

can just use without necessarily being a machine learning expert so if you have like photographs you can run them through the vision API and it will read all the text in it and find all the faces and tell you what kind of objects are in it and do all kinds of good stuff and the translation API has really nice high-quality translations that might be useful lots of things one final closing thing we've also been experimenting with machine learning for doing higher performance machine learning models and so in this case what

we've been doing is a similar kind of reinforcement learning where we're going to take a abstract integrand we have a bunch of computational devices that we want to run that on say for GPU cards and we say to the RL algorithm we want you to find the placement of tensorflow operations on two devices that makes that model run as fast as possible and nor the current way people do this is they hit ok for GPU cards I'm going to run this part of my graph on GPU card 1 this part on GPU card 2 and that's ok

but it's kind of annoying because it's not something that humans really want to think about and so we're actually able to come up with pretty exotic placements so each color there is a different GPU card and this is a on the left you see a sequence prediction model unrolled in time so different time steps are on different GPU cards which is not kind of counterintuitive to what a human expert would do and this is a image model and but the punchline is they're basically 20% faster than the human expert

placement that people came up with ok so now we're here and we think there's a big opportunity with more compute to actually accelerate a lot of the use of machine learning and the sort of different applications and societal benefits that you can actually get from it so I'm pretty excited about that and you know example queries of the future you know actually the upper-left one we can already answer describe this video

in Spanish I didn't show you but we can actually caption and generate sentences about images it's probably not that long before we'll be able to describe a human video findme documents related to reinforcement learning for robotics and summarizes them in German you know that's pretty complicated request but imagine how pert that's the kind of thing you would give to an undergraduate as like a semester project and then please come back with a report for me but imagine if we could actually do that how much more productive everyone would

be be pretty amazing and then robotics I think is at a inflection point where through machine learning for control we're going to have robots that can actually operate in messy environments like this one or the kitchen over there and actually know how to manipulate things in a safe way interacting with humans so that's going to be exciting too so you already know this but deep nets are making big changes and you should pay attention you can find more info about our work at gzo

slash brain and oh I am you could join our team but you're already starting company before we get to questions I have a poll that requested me to do and I'm curious too how many of you are using deep learning models in what you're doing okay ah how many of you are using cafe how many of you are using high torque and a

half-hearted PI torch how many how many are using piano carols okay and tensors Wow okay cool that's good to know we're excellent yes yes roughly in proportion in fact ah cool ah anything to add Zach okay any questions yeah well I when you talk about sort of

the learning to learn stuff into the neural net models designing other neural net models like for example when the neural net model designed a model that perform better on C 410 and other models do you look at those models and say oh I understand why that performs better or is it kind of the case that it did something wacky and you don't understand really why it works better I mean I think it depends like some sometimes you just want the most accurate end model for the problem you care about and that's fine sometimes you're trying

to come up with a model and you want to understand why it's more accurate so that you can then drive further human oriented machine learning research so I think it depends like the symbolic expressions for the optimizer update rule those are actually pretty interpretive all so like if I go back to the it's acts pretty interesting right if you look here there's this sub expression e to the sign of the gradient

times the sign of the momentum that seems to reoccur in a lot of these different optimizers that it's learned and that sort of makes sense basically if the sign is the same as the direction you've been going then speed up and if it's different then slow way down right and that's kind of a good intuition to have and you can see that the reinforcement learning wanted to do that in like five of these things so in some sense depending on what problem you set up and we'll learn to

learn framework you can actually come up with human insights about oh well that makes sense from the experiments of Threatened but you know here that that you can kind of investigate that cell and understand if you actually look it's doing a bunch of ads at the bottom but it's also doing an element-wise multiply for the input data in for one of the paths through the cell which is kind of different from the lsdm cells doing and so that might be sort of insight into

why is doing that if you look here you know I think that architecture is kind of crazy but we do know from res network that these skipped connections make a lot of sense and so this is just kind of like crazy skip connections and lots of places oh yeah then I guess maybe a solid question is do you think this is going to be a tool for humans to build better nets or this is going to be how Nets are built in the futures with other ah could be both but I will say that this system can run twelve thousand

experiments in a weekend and humans are not that good at that so with all that compute you are showing it strikes me that you might run out of human trainable data is that stuff really for the reinforcement learning where you can run 12,000 experiments in a weekend or do you have enough human labeled data - oh so for example I that amount on computation when we were training our translation models for one

language pair we were using hundreds of GPUs for a week and for that problem we actually have enough training data that we could only get through 1/6 of that data once so we know that if we could get through all of it the quality would be way better right because that's just a general rule of machine learning if you could get through all your data probably it'd be better than not and if you could even go through it a few times I would value even better so we think there are plenty of problems where there's enough labeled data in the world that you want to tackle a single problem

and a train a single model on something like that but it's also going to be pretty good for a small model exploration where you try to you know ten thousand different things maybe take an hour to run on some subset of the chips it just depends with the problem you know the architecture search is kind of tenable with not current generation but one generation to go GPUs for things like C far 10 because you run that for an hour and you get an answer for one of the experiments and you run twelve thousand of those so 700 GPUs over a

weekend we know there's a bunch of algorithmic improvements we could do to drop that by factor 10 but it's kind of just on the boundary of practical for tiny problems and making it practical for real problems at scale I think you're going to be really really cool maybe it may be a follow-up question of that so you had that slide on there we have person data compute persons gone with data do you see anything in the near term in which you could have really

powerful models on very very small much smaller data sets than a company like Google what it would have access to yeah I mean I think the right way to tackle that is right now the way we as a community tackle learning problems as we say okay we're going to train a model to do this and we might say gee we don't have much data for this problem we're going to do transfer learning from imagenet and then I have my 5000 flower images and I'm going to do transfer learning and fine tuning on that but

that's really kind of lame right like we want to build real systems real intelligent systems we want a model that knows how to do a thousand things ten thousand things and then when the ten thousand first thing comes along we want it to build on its knowledge for how to solve those ten thousand things so that it can solve the ten thousand the first thing with much less data with many fewer examples with building on the representations that's already learned so if we can build a single giant model that can do thousands of things that's going to improve the data efficiency

problem a lot and also the time to the wall time to actually be able to master a new task problem as well so I think that's the way we're going to get to you know more data efficient more flexible things because the problem with the current approach is we train a model do one thing and then it can't do anything else which is pretty mean what do your best engineers do while they're waiting for model to learn

well they often start up other experiments and hit reload on the visualizer they write code they think of ideas at a whiteboard they do lots of things but you know getting that that's like alliteration time down from you know days or weeks to hours really just qualitatively changes your workflow and so I think we're really shooting for making that time to result as well as

possible and then they won't have you know people will not have these these week-long things whether you know gosh I hope my experiment works so what would you attribute the gap in translation quality to between languages is it just amount of data behind each one I think some language pairs the the translations are more natural because they're more related kinds of language families and the alignment is maybe the similar as opposed to a very different word order and very different character sets for

example but I think ultimately we will get higher accuracy models by you know using a pod to train a really big model and get through all the data once I suspect we could probably exceed human quality translations for some language pairs you know if we get through all the data once maybe that may be a slightly bigger model and the analogy is you know even a the best human translator is only going to see so many words in their life and if your translation system can train

a lot more data and see more of them even though it's probably not as intelligent and flexible at getting maximal information for each word that it sees it maybe at some point gonna do better [Music] so to be honest we haven't experimented with a broad enough set of tasks to really make conclusions here I suspect

that there may be tasks I think probably for any supervised tasks that like where you have a crisply defined input and output and you have enough training data you know it'll probably work it's a question of how much compute you need to apply we have a lot of ideas around you know making the algorithmic search more efficient by cutting off experiments that are obviously dead early rather than running from to conclusion doing lots of things like that I think the architecture search itself

right now we train a bespoke model generating model for each problem we're trying to solve and so obviously you'd want to train a model generating models that solves many problems and then you'll be able to get in a better state of good architectures for a new problem because of seeing similar problems and you're like oh yeah lots of convolutions and 12 layers is a good place to start or something how does the internal development cycle look like for

optimizing the the machinery involved like Calvin you refrain how often do you play with the different set of pepper grinders for the learn to learn models in particular for example for all the free training API to provide life for this API how frequently we kind of do everything right ah it varies depending on the domain like some domains like vision are pretty stable like you don't need to retrain every

every hour but other domains like for some of our internal problems like predicting you know you know maybe you're trying to predict what ads are relevant that set changes fairly rapidly like there's a new chocolate festival on Long Island tomorrow that wasn't there yesterday and now you actually want to know that that's important so some things have a very stable distribution some don't it really does varies a lot depending the problem certainly it's easier for

things like speech or vision we're just the basic perception is what you're trying to do and the distribution is pretty stable and if you have a changing distribution that just introduces lots of annoying production issues because now you have to retrain and you need to sort of somehow integrate new data so that you can learn new concepts and new new correlations relatively quickly so that you can then produce good correlation good output you mentioned

sort of fast iteration being really important for developing this stuff how much of the process sort of now and like the cutting edge you know neural net development is still trial and error and how much of it is like I'm going to insert this and I know what's going to happen I mean I think a lot of machine learning research is empirical these days right you have an idea you think it'll work but you need to try to implement it try it on interesting

problems explore the set of hyper parameters or whatever that will make the idea go from not working to hopefully working and so it's often the case that you need to do this kind of impaired stuff I mean there's some ideas that you have a lot of intuition like oh yeah that's definitely going to work even beforehand because it's sort of putting together two things that did work with a third thing that also did work and it seems pretty obvious that combining them is going to work as well but other things it's hard to build the intuition

no a while ago you guys have done some really great work on helping visualize what a convolutional neural network doing image classification was doing so like the interpretability of models seem to be like a focus for a bit and then there's a point where you kind of cross that and it's like to learn to learn stuff you just can't interpret and maybe this is kind of what you're asking but how important is that for delivering production models to humans who maybe are not machine learning experts that need to work alongside a robot classifier it's really important in some

domains and not important in others and we actually have a pretty big focus I have a much longer talk or a set of slides I select a subset among we we have we were doing about to work in sort of understanding and visualizing and gilding interpretability for models that i didn't talk about but it is an important area the main areas where I think it's really important are in health care if you tell someone you're providing advice to a physician you say patient needs a heart valve replacement

you know they're going to want to know why are you saying this right and so if you can go back and highlight a part of a medical note that says you know a year and a half ago a patient was complaining about like their hearts felt like it skipped a beat every so often or something that's going to give much more smooth interactions between a machine learning system and a human and let them kind of each do play to their strengths whereas if you just given black box prediction that's often not as useful in

some domains but some things like image classification I just want the most accurate image possible classification puzzle yeah coffee consumption lotion six years ago and is there anything to be learned from that five other areas and they have an explosive growth feature so the thing that caused me to start doing this as I heard I kind of like to keep a pulse on

different areas of designs and I started to see neural that's being successful in some domains just by like reading abstracts of things and I had this bug in the back of my head from my undergrad thesis that neural nets were actually the right abstraction and so I kind of heard Inklings I chatted with Andrew Inge who was consulting at Google one day a week and he said oh yeah no nuts or sir I'm like what do you work out at Stanford he's like oh no that's our kind of interesting again i'ma go cool really I used to do work on that and I kind of

he and I started talking and I just felt like if the problem for neural nets was scaled like back from my experience where you know more compute seemed like the right answer but wasn't then but now we actually have a lot more data we have much more compute in a single processor but if we could throw lots of processors at this problem then perhaps scale would let us solve problems that we couldn't before and so I kind of had this inkling that neural nets were great abstraction

from 20 years before and felt like it would be fun to go see if we could make them scale

right I mean I think there's probably a lot of algorithmic things that we're going to need but I do think one of the major problems and why we don't have systems that appear to reason is because of this problem of training neural nets to do one thing right if you had a lot more compute and you had a model that could do tens of thousands of things and you had some algorithmic constructs where you sat there and cogitated for a while and built sort of plausible scenarios and explored them with compute

computation and then eventually came back with an answer for this new thing that might appear to be more like reasoning because you're building on all this other groundwork of knowledge that you've learned from solving ten thousand other things and I think that's what humans do right we we learned to do a new task or to reason through something by building on our experience that we've already accumulated from you know perception and building up that kind of low-level thing around the world but

also from you know bringing concepts together from math and science through our education and being able to reason through something so I think you know a lot of it is that we don't have these massively multitask models being trained today [Music] the knowledge store that [Music] right I didn't put in this talk but I think one of the real problems that we have is we kind of have a model and we

densely activate the entire model for everything we do I think what we actually want is a model that's very very big like think you know 100 billion trillion parameters but where for any given thing you activate only a tiny fraction of it 1% of it 5% of it you know your brain works this way and that seems like and you have to be able to the way you store stuff is just by having a lot of parameters

the time network nearly 10,000 sure I mean I think memory networks are kind of an interesting emerging area where you have this kind of local state that you can update and mutate in the process of accomplishing some sort of task so far I

think those have been applied to relatively modest size problems they may be part of the solution part of something that we would want like short-term working memory as you're like looking through a set of possible solutions to some problem I think you know they're definitely an interesting area I think combining that with you know a model that does ten thousand things or a million things might get us pretty far

second Simon Maybelline well I think one of the nice things about architecture searches actually combines really well with machine learning researchers so if someone comes up with a new interesting thing you can put that in the search space of these automated learning to learn systems pretty easily and then all of a sudden you now have the access to this hybrid best of both worlds like very primitive things but also these

hand design things that humans have come up with the team effective and use that as the the search day so it's not like machine learning researchers will not have anything to do and I think you know there's a ton of work in figuring out what are interesting problems where machine learning can actually make a difference and which ones are worth solving and how do we solve them so maybe good time for this question what

what's in your opinion what's like the coolest thing neural nets are being applied to you right now I'm really excited about healthcare I think the ability of neural nets to ingest a lot of data and then make sort of interesting predictions in a smooth way that you can take a patient in a particular state and say okay here are the five most likely diagnosis for the station because you know I've seen you know a million other patients and I I

know the 17 you need to have similar conditions you know I think that's that's one that we'll have a really big societal impact it's you know fraught with lots of role lot issues because there's a heavily regulated environment there's all kinds of privacy issues but ultimately I think making better healthcare decisions is going to be pretty big the coolest things you know I really like all the art generation kind of wings those are fun the ability of

neural nets to write a sentence about an image you know I I was kind of surprised that happened that early I would have said you know before that work some of which was done in our group I would have said I don't you know we're good at saying that the lion I don't think we can say that's the lion sleeping on a rock with a pretty yellow Ming or whatever it is and that that's pretty cool okay okay anywhere thank you [Applause]

you
